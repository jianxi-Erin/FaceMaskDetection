# %%
import os
import shutil
import random
import xml.etree.ElementTree as ET
from pathlib import Path

# ===============================
# ğŸ§± é…ç½®å‚æ•°åŒºåŸŸ
# ===============================
DATASET_URL = "andrewmvd/face-mask-detection"  # kaggle æ•°æ®é›†åç§°
DATASET_DIR = "dataset"        # è§£å‹æ•°æ®é›†çš„è·¯å¾„
ANNOT_DIR = os.path.join(DATASET_DIR, "annotations")
IMAGES_DIR = os.path.join(DATASET_DIR, "images")

OUTPUT_DIR = "dataset/output"
TRAIN_DIR = os.path.join(OUTPUT_DIR, "train")
TEST_DIR = os.path.join(OUTPUT_DIR, "test")
YAML_PATH = "voc.yaml"

CLASS_NAMES = ["with_mask", "without_mask","mask_weared_incorrect"]
TRAIN_RATIO = 0.8

# ===============================
# ğŸ“¥ ä¸‹è½½å¹¶è§£å‹æ•°æ®é›†
# ===============================
def download_dataset(dataset_url, output_dir):
    try:
        import kagglehub
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… kagglehubï¼špip install kagglehub")
        return

    if os.path.exists(output_dir):
        print(f"âœ… æ•°æ®é›†å·²å­˜åœ¨ï¼š{os.path.abspath(output_dir)}")
        return

    print("ğŸš€ æ­£åœ¨ä¸‹è½½æ•°æ®é›†ä¸­...")
    downloaded_path = kagglehub.dataset_download(dataset_url)
    shutil.copytree(downloaded_path, output_dir)
    shutil.rmtree(downloaded_path)
    print(f"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼š{os.path.abspath(output_dir)}")

# ===============================
# ğŸ” VOC â†’ YOLO æ ¼å¼è½¬æ¢å‡½æ•°
# ===============================
def convert_voc_to_yolo(xml_file, yolo_save_dir, class_names):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    img_width = int(root.find("size/width").text)
    img_height = int(root.find("size/height").text)
    label_lines = []

    for obj in root.findall("object"):
        cls_name = obj.find("name").text
        if cls_name not in class_names:
            continue
        cls_id = class_names.index(cls_name)

        bbox = obj.find("bndbox")
        xmin = int(bbox.find("xmin").text)
        ymin = int(bbox.find("ymin").text)
        xmax = int(bbox.find("xmax").text)
        ymax = int(bbox.find("ymax").text)

        # è½¬æ¢ä¸ºå½’ä¸€åŒ–ä¸­å¿ƒåæ ‡ + å®½é«˜
        x_center = ((xmin + xmax) / 2) / img_width
        y_center = ((ymin + ymax) / 2) / img_height
        width = (xmax - xmin) / img_width
        height = (ymax - ymin) / img_height

        label_lines.append(f"{cls_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")

    # ä¿å­˜ä¸º .txt æ–‡ä»¶
    txt_file = Path(yolo_save_dir) / (Path(xml_file).stem + ".txt")
    with open(txt_file, "w") as f:
        f.write("\n".join(label_lines))

# ===============================
# ğŸ”€ åˆ’åˆ†æ•°æ®é›†å¹¶æ‰§è¡Œè½¬æ¢
# ===============================
def prepare_dataset(train_ratio):
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs(TRAIN_DIR, exist_ok=True)
    os.makedirs(TEST_DIR, exist_ok=True)

    # åŠ è½½æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶å’Œå¯¹åº”å›¾åƒ
    xml_files = sorted(Path(ANNOT_DIR).glob("*.xml"))
    img_files = [Path(IMAGES_DIR) / (xml.stem + ".png") for xml in xml_files]

    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for img in img_files:
        if not img.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶ï¼š{img}")

    # æ‰“ä¹±å¹¶åˆ’åˆ†æ•°æ®é›†
    data = list(zip(xml_files, img_files))
    random.shuffle(data)
    split_idx = int(len(data) * train_ratio)
    train_data, test_data = data[:split_idx], data[split_idx:]

    def process(data_list, target_dir):
        for xml_path, img_path in data_list:
            shutil.copy(xml_path, target_dir)
            shutil.copy(img_path, target_dir)
            convert_voc_to_yolo(xml_path, target_dir, CLASS_NAMES)

    process(train_data, TRAIN_DIR)
    process(test_data, TEST_DIR)

    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼Œè®­ç»ƒé›†æ•°é‡ï¼š{len(train_data)}ï¼Œæµ‹è¯•é›†æ•°é‡ï¼š{len(test_data)}")

# ===============================
# ğŸ“ ç”Ÿæˆ YOLO YAML é…ç½®æ–‡ä»¶
# ===============================
def create_yaml(path, train_dir, val_dir, class_names):
    with open(path, "w") as f:
        f.write(f"train: {os.path.abspath(train_dir)}\n")
        f.write(f"val: {os.path.abspath(val_dir)}\n\n")
        f.write(f"nc: {len(class_names)}\n")
        f.write(f"names: {class_names}\n")
        f.write(f"weights: [1.0, 2.0, 4.0]  # æ ¹æ®å„ç±»åˆ«æ ·æœ¬é‡è®¾ç½®é€†æƒé‡\n")
        f.write(f"sample_weights: True       # å¯ç”¨åŠ æƒéšæœºé‡‡æ ·\n")
    print(f"ğŸ“„ å·²ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼š{path}")

# ===============================
# ğŸš€ ä¸»æ‰§è¡Œå…¥å£
# ===============================

download_dataset(DATASET_URL, DATASET_DIR)
prepare_dataset(TRAIN_RATIO)
create_yaml(YAML_PATH, TRAIN_DIR, TEST_DIR, CLASS_NAMES)


# %%
# !pip install ultralytics
# !nvidia-smi
from ultralytics import YOLO
device = 'cuda'  # ä½¿ç”¨GPUè®­ç»ƒ,å¯é€‰cudaæˆ–cpu

model = YOLO("baseModel/yolov8n.pt")  # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
model.train(
    data="voc.yaml",
    device=0 if device == "cuda" else "cpu",
    epochs=100,                  # å¢åŠ æ€»è®­ç»ƒè½®æ¬¡
    batch=32,                    # å‡å°batch sizeä»¥é€‚åº”æ›´å¤§åˆ†è¾¨ç‡
    imgsz=800,                   # ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼ˆ800->640ï¼‰
    optimizer="AdamW",
    lr0=0.0005,                    # é™ä½åˆå§‹å­¦ä¹ ç‡
    lrf=0.005,                    # ä½™å¼¦é€€ç«æœ€ç»ˆå­¦ä¹ ç‡
    warmup_epochs=3,   # æ–°å¢å­¦ä¹ ç‡é¢„çƒ­
    weight_decay=0.05,           # æ·»åŠ æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ

    # æ•°æ®å¢å¼ºå¼ºåŒ–
    augment=True,
    hsv_h=0.3,                   # å¢å¼ºè‰²è°ƒæ‰°åŠ¨
    hsv_s=0.6,                   # å¢å¼ºé¥±å’Œåº¦æ‰°åŠ¨
    translate=0.2,               # å¢å¤§å¹³ç§»å¹…åº¦
    scale=0.5,                   # æ‰©å¤§ç¼©æ”¾èŒƒå›´
    shear=0.3,                   # å¢å¤§å‰ªåˆ‡å¹…åº¦
    mosaic=1.0,                  # å…¨ç¨‹å¼€å¯mosaic
    close_mosaic=15,             # æœ€å15ä¸ªepochå…³é—­mosaicç¨³å®šè®­ç»ƒ

    # æŸå¤±å‡½æ•°è°ƒæ•´
    cls=3.0,                     # å¢å¤§åˆ†ç±»æŸå¤±æƒé‡
    box=1.5,                     # å¢å¤§æ¡†å›å½’æŸå¤±æƒé‡
    dfl=1.5,                     # å¢å¤§ç‚¹æ¡†æŸå¤±æƒé‡
    # fl_gamma=2.0,                # èšç„¦å›°éš¾æ ·æœ¬ï¼ˆç±»ä¼¼focal lossï¼‰

    # obj=1.5,                     # é€‚å½“å¢å¤§ç›®æ ‡å­˜åœ¨æŸå¤±æƒé‡
    copy_paste=0.3,              # æ–°å¢å¤åˆ¶ç²˜è´´å¢å¼ºï¼ˆç‰¹åˆ«é’ˆå¯¹å°‘æ•°ç±»ï¼‰
    mixup=0.2,       # æ–°å¢MixUpå¢å¼ºï¼ˆé»˜è®¤æœªå¯ç”¨ï¼‰
    # ç±»åˆ«å¹³è¡¡ç­–ç•¥
    # éœ€åœ¨data.yamlä¸­æ·»åŠ ï¼š
    #   weights: [1.0, 1.0, 2.0, 4.0]  # æŒ‰ç±»åˆ«æ ·æœ¬é‡å€’æ•°è®¾ç½®æƒé‡
    #   sample_weights: True           # å¯ç”¨æ ·æœ¬åŠ æƒé‡‡æ ·

    # è®­ç»ƒæ§åˆ¶
    patience=15,                  # å»¶é•¿æ—©åœè§‚å¯ŸæœŸ
    dropout=0.3,                 # æ·»åŠ Dropoutæ­£åˆ™åŒ–
    amp=True,                    # ä¿æŒæ··åˆç²¾åº¦è®­ç»ƒ
    pretrained=True,
    save=True,
    exist_ok=True,
)

# %%
# é¢„æµ‹è¾“å‡ºï¼ˆä½¿ç”¨OpenCVç»Ÿä¸€ç»˜å›¾ï¼‰
import os
import cv2
import torch
from ultralytics import YOLO
import matplotlib.pyplot as plt

# ------------ å…¨å±€é…ç½® ------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = "runs/detect/train/weights/best.pt"
model = YOLO(MODEL_PATH)
# INPUT_PATH = "dataset/output/video/test.mp4"  # è¾“å…¥è·¯å¾„ï¼Œå¯ä»¥æ˜¯å›¾ç‰‡/è§†é¢‘/æ–‡ä»¶å¤¹/æ‘„åƒå¤´
# INPUT_PATH = "dataset/output/test/"         
INPUT_PATH = 0
SAVE = True  # æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ
OUTPUT_PATH = "predict/"  # é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„

# ------------ å·¥å…·å‡½æ•° ------------

# ä½¿ç”¨OpenCVç»˜åˆ¶è¯†åˆ«æ¡†å’Œæ–‡å­—
# ä½¿ç”¨ OpenCV ç»˜åˆ¶æ£€æµ‹ç»“æœï¼ˆæ ¹æ®ç±»åˆ«åŠ¨æ€æ¢é¢œè‰²ï¼‰
def draw_boxes_cv2(image, results):
    for box in results[0].boxes:
        # 1. å–å‡ºæ£€æµ‹æ¡†çš„åæ ‡ï¼Œå¹¶å››èˆäº”å…¥ä¸ºæ•´æ•°
        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
        cls_id = int(box.cls)   # è·å–ç±»åˆ«ID
        conf = float(box.conf)  # è·å–ç½®ä¿¡åº¦
        label_name = model.names[cls_id]  # è·å–ç±»åˆ«åç§°
        label = f"{label_name} {conf:.2f}"  # ç”Ÿæˆæ˜¾ç¤ºæ ‡ç­¾æ–‡æœ¬

        # 2. æ ¹æ®ç±»åˆ«åç§°å†³å®šç»˜åˆ¶é¢œè‰²
        if label_name == "with_mask":
            color = (0, 255, 0)      # ç»¿è‰² (BGR)
        elif label_name == "without_mask":
            color = (0, 0, 255)      # çº¢è‰² (BGR)
        else:
            color = (0, 165, 255)    # æ©™è‰² (BGR)

        # 3. ç»˜åˆ¶çŸ©å½¢æ¡†ï¼ˆæ¡†é¢œè‰²æ ¹æ®ç±»åˆ«å˜åŒ–ï¼‰
        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)

        # 4. ç»˜åˆ¶æ–‡å­—èƒŒæ™¯çŸ©å½¢ï¼ˆå¡«å……èƒŒæ™¯è‰²ï¼‰
        (text_w, text_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        cv2.rectangle(image, (x1, y1 - text_h - 4), (x1 + text_w, y1), color, -1)

        # 5. ç»˜åˆ¶ç™½è‰²æ–‡å­—ï¼ˆå§‹ç»ˆä¸ºç™½è‰²ï¼‰
        cv2.putText(image, label, (x1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

    return image


# ä¿å­˜å›¾ç‰‡
def save_image_cv2(image, save_path, origin_path=None):
    if os.path.isdir(save_path):
        filename = os.path.basename(origin_path)
        save_path = os.path.join(save_path, filename)
    else:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
    cv2.imwrite(save_path, image)
    print(f"âœ… å·²ä¿å­˜å›¾ç‰‡: {save_path}")

# ------------ å•å¼ å›¾ç‰‡é¢„æµ‹ ------------
def predict_image(image_path, save=False, save_path=None):
    image = cv2.imread(image_path)
    results = model.predict(image_path, imgsz=640, device=DEVICE)
    image = draw_boxes_cv2(image, results)

    # æ˜¾ç¤ºç»“æœ
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    plt.imshow(image_rgb)
    plt.axis("off")
    plt.title("é¢„æµ‹ç»“æœ")
    plt.show()

    # ä¿å­˜ç»“æœ
    if save and save_path:
        save_image_cv2(image, save_path, origin_path=image_path)

# ------------ è§†é¢‘æ–‡ä»¶é¢„æµ‹ ------------
def predict_video(video_path, save=False, save_path=None):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ è§†é¢‘æ–‡ä»¶æ— æ³•æ‰“å¼€")
        return

    if save:
        if os.path.isdir(save_path):
            filename = os.path.basename(video_path)
            save_path = os.path.join(save_path, f"{os.path.splitext(filename)[0]}.mp4")
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        fps, w, h = cap.get(5), int(cap.get(3)), int(cap.get(4))
        out = cv2.VideoWriter(save_path, fourcc, fps, (w, h))

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = model.predict(frame, imgsz=640, device=DEVICE,verbose=False)
        frame = draw_boxes_cv2(frame, results)

        cv2.imshow("é¢„æµ‹ä¸­ - æŒ‰ Q é€€å‡º", frame)
        if save:
            out.write(frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    if save:
        out.release()
        print(f"âœ… å·²ä¿å­˜è§†é¢‘: {save_path}")
    cv2.destroyAllWindows()

# ------------ æ–‡ä»¶å¤¹æ‰¹é‡å›¾ç‰‡é¢„æµ‹ ------------
# æ–‡ä»¶å¤¹æ‰¹é‡é¢„æµ‹ + å®æ—¶è¿›åº¦æ¡
def predict_folder(folder_path, save=False, output_dir=None):
    # è·å–å…¨éƒ¨å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
    all_files = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith((".jpg", ".jpeg", ".png", ".bmp", ".tiff")):
                all_files.append(os.path.join(root, file))

    total_files = len(all_files)  # æ€»æ–‡ä»¶æ•°
    if total_files == 0:
        print("âŒ æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
        return

    # éå†æ¯ä¸€å¼ å›¾ç‰‡è¿›è¡Œé¢„æµ‹
    for idx, img_path in enumerate(all_files, start=1):
        image = cv2.imread(img_path)
        results = model.predict(img_path, imgsz=640, device=DEVICE,verbose=False)
        image = draw_boxes_cv2(image, results)

        if save and output_dir:
            rel_path = os.path.relpath(img_path, folder_path)
            save_path = os.path.join(output_dir, rel_path)
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            cv2.imwrite(save_path, image)

        # --- ç»˜åˆ¶è¿›åº¦æ¡ ---
        progress = idx / total_files
        bar_len = 30  # è¿›åº¦æ¡é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
        filled_len = int(bar_len * progress)
        bar = "â–ˆ" * filled_len + "-" * (bar_len - filled_len)
        print(f"\rğŸ”„ é¢„æµ‹è¿›åº¦: [{bar}] {progress*100:.1f}% ({idx}/{total_files})", end="")

    # æœ€åæ¢è¡Œ
    print()

    if save:
        print(f"âœ… æ–‡ä»¶å¤¹é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_dir}")


# ------------ æ‘„åƒå¤´å®æ—¶é¢„æµ‹å¹¶å¯ä¿å­˜å½•åƒ ------------
def predict_camera(index=0, save=False, output_dir="predict/"):
    cap = cv2.VideoCapture(index)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {index}")
        return

    # å¦‚æœéœ€è¦ä¿å­˜è§†é¢‘ï¼Œåˆå§‹åŒ– VideoWriter
    if save:
        import datetime
        now = datetime.datetime.now()
        timestamp = now.strftime("%Y%m%d%H%M%S")  # è·å–å½“å‰æ—¶é—´ï¼šå¹´æœˆæ—¥æ—¶åˆ†ç§’ï¼Œçº¯æ•°å­—
        os.makedirs(output_dir, exist_ok=True)   # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        save_path = os.path.join(output_dir, f"camera_{index}_{timestamp}.mp4")

        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps == 0 or fps is None:  # æœ‰äº›æ‘„åƒå¤´å¯èƒ½å–ä¸åˆ°å¸§ç‡
            fps = 30  # é»˜è®¤è®¾ä¸º30å¸§
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        out = cv2.VideoWriter(save_path, fourcc, fps, (width, height))

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = model.predict(frame, imgsz=640, device=DEVICE)
        frame = draw_boxes_cv2(frame, results)

        # æ˜¾ç¤ºé¢„æµ‹ç”»é¢
        cv2.imshow("æ‘„åƒå¤´é¢„æµ‹ - æŒ‰ Q é€€å‡º", frame)

        # ä¿å­˜é¢„æµ‹ç”»é¢
        if save:
            out.write(frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    if save:
        out.release()
        print(f"âœ… æ‘„åƒå¤´è§†é¢‘å·²ä¿å­˜åˆ°: {save_path}")
    cv2.destroyAllWindows()


# ------------ æ€»å…¥å£å‡½æ•° ------------
def run_predict(path, save=False, save_path=None):
    if isinstance(path, int):
        predict_camera(index=path,save=save,output_dir=save_path)
    elif os.path.isfile(path):
        ext = os.path.splitext(path)[1].lower()
        if ext in [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]:
            predict_image(path, save, save_path)
        elif ext in [".mp4", ".avi", ".mov", ".mkv"]:
            predict_video(path, save, save_path)
    elif os.path.isdir(path):
        predict_folder(path, save, save_path)
    else:
        print("âŒ æ— æ•ˆè·¯å¾„ï¼Œè¯·ç¡®è®¤è¾“å…¥æ­£ç¡®çš„å›¾ç‰‡/è§†é¢‘/æ–‡ä»¶å¤¹/æ‘„åƒå¤´ç¼–å·")

# ------------ ç¤ºä¾‹è°ƒç”¨ ------------
run_predict(INPUT_PATH, SAVE, OUTPUT_PATH)



