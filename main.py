# %%
import os
import shutil
import random
import xml.etree.ElementTree as ET
from pathlib import Path

# ===============================
# ğŸ§± é…ç½®å‚æ•°åŒºåŸŸ
# ===============================
DATASET_URL = "andrewmvd/face-mask-detection"  # kaggle æ•°æ®é›†åç§°
DATASET_DIR = "dataset"        # è§£å‹æ•°æ®é›†çš„è·¯å¾„
ANNOT_DIR = os.path.join(DATASET_DIR, "annotations")
IMAGES_DIR = os.path.join(DATASET_DIR, "images")

OUTPUT_DIR = "dataset/output"
TRAIN_DIR = os.path.join(OUTPUT_DIR, "train")
TEST_DIR = os.path.join(OUTPUT_DIR, "test")
YAML_PATH = "voc.yaml"

CLASS_NAMES = ["with_mask", "without_mask","mask_weared_incorrect"]
TRAIN_RATIO = 0.8

# ===============================
# ğŸ“¥ ä¸‹è½½å¹¶è§£å‹æ•°æ®é›†
# ===============================
def download_dataset(dataset_url, output_dir):
    try:
        import kagglehub
    except ImportError:
        print("âŒ è¯·å…ˆå®‰è£… kagglehubï¼špip install kagglehub")
        return

    if os.path.exists(output_dir):
        print(f"âœ… æ•°æ®é›†å·²å­˜åœ¨ï¼š{os.path.abspath(output_dir)}")
        return

    print("ğŸš€ æ­£åœ¨ä¸‹è½½æ•°æ®é›†ä¸­...")
    downloaded_path = kagglehub.dataset_download(dataset_url)
    shutil.copytree(downloaded_path, output_dir)
    shutil.rmtree(downloaded_path)
    print(f"âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼š{os.path.abspath(output_dir)}")

# ===============================
# ğŸ” VOC â†’ YOLO æ ¼å¼è½¬æ¢å‡½æ•°
# ===============================
def convert_voc_to_yolo(xml_file, yolo_save_dir, class_names):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    img_width = int(root.find("size/width").text)
    img_height = int(root.find("size/height").text)
    label_lines = []

    for obj in root.findall("object"):
        cls_name = obj.find("name").text
        if cls_name not in class_names:
            continue
        cls_id = class_names.index(cls_name)

        bbox = obj.find("bndbox")
        xmin = int(bbox.find("xmin").text)
        ymin = int(bbox.find("ymin").text)
        xmax = int(bbox.find("xmax").text)
        ymax = int(bbox.find("ymax").text)

        # è½¬æ¢ä¸ºå½’ä¸€åŒ–ä¸­å¿ƒåæ ‡ + å®½é«˜
        x_center = ((xmin + xmax) / 2) / img_width
        y_center = ((ymin + ymax) / 2) / img_height
        width = (xmax - xmin) / img_width
        height = (ymax - ymin) / img_height

        label_lines.append(f"{cls_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}")

    # ä¿å­˜ä¸º .txt æ–‡ä»¶
    txt_file = Path(yolo_save_dir) / (Path(xml_file).stem + ".txt")
    with open(txt_file, "w") as f:
        f.write("\n".join(label_lines))

# ===============================
# ğŸ”€ åˆ’åˆ†æ•°æ®é›†å¹¶æ‰§è¡Œè½¬æ¢
# ===============================
def prepare_dataset(train_ratio):
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs(TRAIN_DIR, exist_ok=True)
    os.makedirs(TEST_DIR, exist_ok=True)

    # åŠ è½½æ‰€æœ‰æ ‡æ³¨æ–‡ä»¶å’Œå¯¹åº”å›¾åƒ
    xml_files = sorted(Path(ANNOT_DIR).glob("*.xml"))
    img_files = [Path(IMAGES_DIR) / (xml.stem + ".png") for xml in xml_files]

    # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for img in img_files:
        if not img.exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶ï¼š{img}")

    # æ‰“ä¹±å¹¶åˆ’åˆ†æ•°æ®é›†
    data = list(zip(xml_files, img_files))
    random.shuffle(data)
    split_idx = int(len(data) * train_ratio)
    train_data, test_data = data[:split_idx], data[split_idx:]

    def process(data_list, target_dir):
        for xml_path, img_path in data_list:
            shutil.copy(xml_path, target_dir)
            shutil.copy(img_path, target_dir)
            convert_voc_to_yolo(xml_path, target_dir, CLASS_NAMES)

    process(train_data, TRAIN_DIR)
    process(test_data, TEST_DIR)

    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼Œè®­ç»ƒé›†æ•°é‡ï¼š{len(train_data)}ï¼Œæµ‹è¯•é›†æ•°é‡ï¼š{len(test_data)}")

# ===============================
# ğŸ“ ç”Ÿæˆ YOLO YAML é…ç½®æ–‡ä»¶
# ===============================
def create_yaml(path, train_dir, val_dir, class_names):
    with open(path, "w") as f:
        f.write(f"train: {os.path.abspath(train_dir)}\n")
        f.write(f"val: {os.path.abspath(val_dir)}\n\n")
        f.write(f"nc: {len(class_names)}\n")
        f.write(f"names: {class_names}\n")
        f.write(f"weights: [1.0, 2.0, 4.0]  # æ ¹æ®å„ç±»åˆ«æ ·æœ¬é‡è®¾ç½®é€†æƒé‡\n")
        f.write(f"sample_weights: True       # å¯ç”¨åŠ æƒéšæœºé‡‡æ ·\n")
    print(f"ğŸ“„ å·²ç”Ÿæˆé…ç½®æ–‡ä»¶ï¼š{path}")

# ===============================
# ğŸš€ ä¸»æ‰§è¡Œå…¥å£
# ===============================

download_dataset(DATASET_URL, DATASET_DIR)
prepare_dataset(TRAIN_RATIO)
create_yaml(YAML_PATH, TRAIN_DIR, TEST_DIR, CLASS_NAMES)


# %%
# !pip install ultralytics
# !nvidia-smi
from ultralytics import YOLO
device = 'cuda'  # ä½¿ç”¨GPUè®­ç»ƒ,å¯é€‰cudaæˆ–cpu

model = YOLO("baseModel/yolov8n.pt")  # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
model.train(
    data="voc.yaml",
    device=0 if device == "cuda" else "cpu",
    epochs=100,                  # å¢åŠ æ€»è®­ç»ƒè½®æ¬¡
    batch=32,                    # å‡å°batch sizeä»¥é€‚åº”æ›´å¤§åˆ†è¾¨ç‡
    imgsz=800,                   # ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ï¼ˆ800->640ï¼‰
    optimizer="AdamW",
    lr0=0.0005,                    # é™ä½åˆå§‹å­¦ä¹ ç‡
    lrf=0.005,                    # ä½™å¼¦é€€ç«æœ€ç»ˆå­¦ä¹ ç‡
    warmup_epochs=3,   # æ–°å¢å­¦ä¹ ç‡é¢„çƒ­
    weight_decay=0.05,           # æ·»åŠ æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ

    # æ•°æ®å¢å¼ºå¼ºåŒ–
    augment=True,
    hsv_h=0.3,                   # å¢å¼ºè‰²è°ƒæ‰°åŠ¨
    hsv_s=0.6,                   # å¢å¼ºé¥±å’Œåº¦æ‰°åŠ¨
    translate=0.2,               # å¢å¤§å¹³ç§»å¹…åº¦
    scale=0.5,                   # æ‰©å¤§ç¼©æ”¾èŒƒå›´
    shear=0.3,                   # å¢å¤§å‰ªåˆ‡å¹…åº¦
    mosaic=1.0,                  # å…¨ç¨‹å¼€å¯mosaic
    close_mosaic=15,             # æœ€å15ä¸ªepochå…³é—­mosaicç¨³å®šè®­ç»ƒ

    # æŸå¤±å‡½æ•°è°ƒæ•´
    cls=3.0,                     # å¢å¤§åˆ†ç±»æŸå¤±æƒé‡
    box=1.5,                     # å¢å¤§æ¡†å›å½’æŸå¤±æƒé‡
    dfl=1.5,                     # å¢å¤§ç‚¹æ¡†æŸå¤±æƒé‡
    # fl_gamma=2.0,                # èšç„¦å›°éš¾æ ·æœ¬ï¼ˆç±»ä¼¼focal lossï¼‰

    # obj=1.5,                     # é€‚å½“å¢å¤§ç›®æ ‡å­˜åœ¨æŸå¤±æƒé‡
    copy_paste=0.3,              # æ–°å¢å¤åˆ¶ç²˜è´´å¢å¼ºï¼ˆç‰¹åˆ«é’ˆå¯¹å°‘æ•°ç±»ï¼‰
    mixup=0.2,       # æ–°å¢MixUpå¢å¼ºï¼ˆé»˜è®¤æœªå¯ç”¨ï¼‰
    # ç±»åˆ«å¹³è¡¡ç­–ç•¥
    # éœ€åœ¨data.yamlä¸­æ·»åŠ ï¼š
    #   weights: [1.0, 1.0, 2.0, 4.0]  # æŒ‰ç±»åˆ«æ ·æœ¬é‡å€’æ•°è®¾ç½®æƒé‡
    #   sample_weights: True           # å¯ç”¨æ ·æœ¬åŠ æƒé‡‡æ ·

    # è®­ç»ƒæ§åˆ¶
    patience=15,                  # å»¶é•¿æ—©åœè§‚å¯ŸæœŸ
    dropout=0.3,                 # æ·»åŠ Dropoutæ­£åˆ™åŒ–
    amp=True,                    # ä¿æŒæ··åˆç²¾åº¦è®­ç»ƒ
    pretrained=True,
    save=True,
    exist_ok=True,
)

# %%
# é¢„æµ‹è¾“å‡º
import os
import cv2
import torch
from PIL import Image, ImageDraw, ImageFont
from ultralytics import YOLO
import matplotlib.pyplot as plt

# ------------ å…¨å±€é…ç½® ------------
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = "runs/detect/train/weights/best.pt"
model = YOLO(MODEL_PATH)
INPUT_PATH = "dataset/output/test/"  # è¾“å…¥è·¯å¾„,å¯ä»¥ä¸ºå›¾ç‰‡,è§†é¢‘,æ–‡ä»¶å¤¹,æ‘„åƒå¤´ç¼–å·
# INPUT_PATH = "dataset/output/video/test.mp4"  # è¾“å…¥è·¯å¾„,å¯ä»¥ä¸ºå›¾ç‰‡,è§†é¢‘,æ–‡ä»¶å¤¹,æ‘„åƒå¤´ç¼–å·
# INPUT_PATH=0

SAVE = True  # æ˜¯å¦ä¿å­˜é¢„æµ‹ç»“æœ
OUTPUT_PATH = "predict/"  # é¢„æµ‹ç»“æœä¿å­˜è·¯å¾„

# ------------ å·¥å…·å‡½æ•° ------------
def draw_boxes_pil(image, results):
    draw = ImageDraw.Draw(image)
    try:
        font = ImageFont.truetype("arial.ttf", 20)
    except:
        font = ImageFont.load_default()

    for box in results[0].boxes:
        x1, y1, x2, y2 = box.xyxy[0].tolist()
        cls_id = int(box.cls)
        conf = float(box.conf)
        label = f"{model.names[cls_id]} {conf:.2f}"

        text_bbox = font.getbbox(label)
        text_w, text_h = text_bbox[2] - text_bbox[0], text_bbox[3] - text_bbox[1]
        draw.rectangle([x1, y1, x2, y2], outline="red", width=2)
        draw.rectangle([x1, y1 - text_h, x1 + text_w, y1], fill="red")
        draw.text((x1, y1 - text_h), label, fill="white", font=font)

    return image

def save_image(image, save_path, origin_path=None):
    if os.path.isdir(save_path):
        filename = os.path.basename(origin_path)
        save_path = os.path.join(save_path, filename)
    else:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
    image.save(save_path)
    print(f"âœ… å·²ä¿å­˜å›¾ç‰‡: {save_path}")

# ------------ å•å›¾é¢„æµ‹ ------------
def predict_image(image_path, save=False, save_path=None):
    image = Image.open(image_path).convert("RGB")
    results = model.predict(image_path, imgsz=640, device=DEVICE)
    image = draw_boxes_pil(image, results)

    plt.imshow(image)
    plt.axis("off")
    plt.title("é¢„æµ‹ç»“æœ")
    plt.show()

    if save and save_path:
        save_image(image, save_path, origin_path=image_path)

# ------------ è§†é¢‘é¢„æµ‹ ------------
def predict_video(video_path, save=False, save_path=None):
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ è§†é¢‘æ–‡ä»¶æ— æ³•æ‰“å¼€")
        return

    if save:
        if os.path.isdir(save_path):
            filename = os.path.basename(video_path)
            save_path = os.path.join(save_path, f"{os.path.splitext(filename)[0]}.mp4")
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        fps, w, h = cap.get(5), int(cap.get(3)), int(cap.get(4))
        out = cv2.VideoWriter(save_path, fourcc, fps, (w, h))

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = model.predict(frame, imgsz=640, device=DEVICE)
        for box in results[0].boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            cls_id = int(box.cls)
            conf = float(box.conf)
            label = f"{model.names[cls_id]} {conf:.2f}"
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)
            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        cv2.imshow("é¢„æµ‹ä¸­ - æŒ‰ Q é€€å‡º", frame)
        if save:
            out.write(frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    if save:
        out.release()
        print(f"âœ… å·²ä¿å­˜è§†é¢‘: {save_path}")
    cv2.destroyAllWindows()

# ------------ æ–‡ä»¶å¤¹æ‰¹é‡å›¾ç‰‡ ------------
def predict_folder(folder_path, save=False, output_dir=None):
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith((".jpg", ".jpeg", ".png", ".bmp", ".tiff")):
                img_path = os.path.join(root, file)
                image = Image.open(img_path).convert("RGB")
                results = model.predict(img_path, imgsz=640, device=DEVICE)
                image = draw_boxes_pil(image, results)

                if save and output_dir:
                    rel_path = os.path.relpath(img_path, folder_path)
                    save_path = os.path.join(output_dir, rel_path)
                    os.makedirs(os.path.dirname(save_path), exist_ok=True)
                    image.save(save_path)

    if save:
        print(f"âœ… æ–‡ä»¶å¤¹é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {output_dir}")

# ------------ æ‘„åƒå¤´å®æ—¶é¢„æµ‹ ------------
def predict_camera(index=0):
    cap = cv2.VideoCapture(index)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {index}")
        return

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        results = model.predict(frame, imgsz=640, device=DEVICE)
        for box in results[0].boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            cls_id = int(box.cls)
            conf = float(box.conf)
            label = f"{model.names[cls_id]} {conf:.2f}"
            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        cv2.imshow("æ‘„åƒå¤´é¢„æµ‹ - æŒ‰ Q é€€å‡º", frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()

# ------------ æ€»å…¥å£å‡½æ•° ------------
def run_predict(path, save=False, save_path=None):
    if isinstance(path, int):
        predict_camera(index=path)
    elif os.path.isfile(path):
        ext = os.path.splitext(path)[1].lower()
        if ext in [".jpg", ".jpeg", ".png", ".bmp", ".tiff"]:
            predict_image(path, save, save_path)
        elif ext in [".mp4", ".avi", ".mov", ".mkv"]:
            predict_video(path, save, save_path)
    elif os.path.isdir(path):
        predict_folder(path, save, save_path)
    else:
        print("âŒ æ— æ•ˆè·¯å¾„ï¼Œè¯·ç¡®è®¤è¾“å…¥æ­£ç¡®çš„å›¾ç‰‡/è§†é¢‘/æ–‡ä»¶å¤¹/æ‘„åƒå¤´ç¼–å·")

# ------------ ç¤ºä¾‹è°ƒç”¨ ------------
run_predict(INPUT_PATH, SAVE, OUTPUT_PATH)      # é¢„æµ‹è¾“å‡º


